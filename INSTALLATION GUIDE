# üöÄ Social Media Analytics Pipeline (Ubuntu Guide)

### A Project by Soulaymane Boulaich (1st Year Big Data & AI Engineering)

As first-year students in **Big Data and AI**, I wanted to build a project that combines both of our core subjects. This project isn't just one simple script‚Äîit's a complete, **end-to-end data system** that does the job of a real data engineer and AI specialist.

-----

## 1\. The "Big Data" Part: The ETL Pipeline

The foundation of this project is a real **ETL (Extract, Transform, Load) pipeline**.

  * **Extract:** The pipeline connects to multiple, live APIs (Reddit, Twitter/X, and YouTube) to pull in messy, raw data (posts, tweets, and comments).
  * **Transform:** It then uses `pandas` to clean, process, and standardize all this different data in one clean, useful format‚Äîall in memory.
  * **Load:** Finally, it loads all the processed data directly into a central `SQLite` database, ready for analysis.

## 2\. The "AI" Part: The Analysis Engine

This is where we add the intelligence. The pipeline takes the clean text and uses **Natural Language Processing (NLP)** to understand it.

  * **Sentiment Analysis:** It uses AI models (`VADER` and a custom-trained `scikit-learn` model) to read the text and classify its emotion as positive, negative, or neutral.
  * **Topic Modeling & NER:** It also uses `spaCy` and `NLTK` to automatically discover the main topics being discussed and find the most mentioned people and organizations.

## 3\. The Result: An Interactive Dashboard

All this data is useless if you can't see it. The final piece is a **web dashboard** built with `Plotly Dash`. It reads directly from our database and lets you:

  * Visualize all the insights in real-time.
  * Filter the data by platform (Reddit, Twitter, etc.).
  * Select a date range to see trends.

This project shows the complete journey: from messy, raw data on the internet to a clean, intelligent, and interactive dashboard.

-----

## üèóÔ∏è Project Architecture

This project is built from **14 key files** that work together as a system.

1.  **Configuration (`.env`, `.gitignore`, `requirements.txt`, `Procfile`)**

      * These files configure the project, hide secrets, and list dependencies.

2.  **Logic Scripts (`collect_*.py`, `database.py`, `process_data.py`, `sentiment_analysis.py`)**

      * These files **contain the functions** for our pipeline (e.g., `def collect_reddit_posts()`). They don't do anything when run directly.

3.  **Runner Scripts (`pipeline.py`, `dashboard.py`, `train_model.py`, `advanced_analysis.py`)**

      * These are the files you **actually run**. They `import` the functions from the logic scripts to perform complex tasks, like running the full ETL or launching the web app.

-----

## üíª Tech Stack

  * **Data Collection:** `praw` (Reddit), `tweepy` (Twitter), `google-api-python-client` (YouTube)
  * **Data Processing:** `pandas`
  * **NLP/ML:** `scikit-learn`, `spacy`, `nltk`, `vaderSentiment`, `joblib`
  * **Database:** `sqlite3`
  * **Dashboard:** `Plotly Dash`
  * **Automation:** `schedule`
  * **Deployment:** `gunicorn`

-----

## üèÉ‚Äç‚ôÇÔ∏è Installation & Setup (Ubuntu Guide)

### 1\. Prerequisites

Open your terminal and make sure you have Python and Git.

```bash
sudo apt update
sudo apt install -y python3 python3-pip python3-venv git
```

### 2\. Clone the Repository

```bash
git clone https://github.com/your_username/your_project_name.git
cd your_project_name
```

### 3\. Create and Activate Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

*(You must run `source venv/bin/activate` every time you open a new terminal)*

### 4\. Install Dependencies

```bash
pip install -r requirements.txt
```

*(This uses the `requirements.txt` file you will create)*

### 5\. Download NLP Models

```bash
python3 -m spacy download en_core_web_sm
python3 -c "import nltk; nltk.download('stopwords'); nltk.download('vader_lexicon')"
```

### 6\. Create `.env` Secrets File

Create a file named `.env` and paste this in. Fill it with your own API keys.

```
# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_secret_here
REDDIT_USER_AGENT=MyAnalyticsApp/1.0

# Twitter/X API Credentials
TWITTER_BEARER_TOKEN=your_bearer_token_here

# YouTube API Credentials
YOUTUBE_API_KEY=your_google_api_key_here
```

-----

## üöÄ How to Run Your Project

### 1\. (Optional) Train the Custom Model

  * Download the [Sentiment140 dataset from Kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140).
  * Unzip it and rename the CSV file to `training_data.csv`.
  * Place it in your project folder and run:
    ```bash
    python3 train_model.py
    ```

*(This will create the `custom_sentiment_model.pkl` file)*

### 2\. Run the ETL Pipeline

This will collect data from all 3 platforms, clean it, analyze it, and save it to `social_media.db`.

```bash
python3 pipeline.py
```

*(Let it run once. When you see "Pipeline scheduled... Press Ctrl+C to stop", press **Ctrl+C**)*

### 3\. Run the Advanced Analysis (Optional)

To see the Topic Modeling and NER results, run:

```bash
python3 advanced_analysis.py
```

### 4\. Run the Dashboard

This starts your web application.

```bash
python3 dashboard.py
```

  * Open your browser and go to **`http://127.0.0.1:8050`** to see your dashboard.

-----

## üöß API Limitations (Facebook/Instagram)

  * Access to the Facebook and Instagram APIs is highly restricted by Meta.
  * It requires a formal Business Verification and an App Review process.
  * This is not feasible for personal or academic projects, as approval is almost exclusively granted to registered businesses for commercial purposes.

-----

-----

# üöÄ Social Media Analytics Pipeline (Windows Guide)

### A Project by Soulaymane Boulaich (1st Year Big Data & AI Engineering)

As first-year students in **Big Data and AI**, I wanted to build a project that combines both of our core subjects. This project isn't just one simple script‚Äîit's a complete, **end-to-end data system** that does the job of a real data engineer and AI specialist.

-----

## 1\. The "Big Data" Part: The ETL Pipeline

The foundation of this project is a real **ETL (Extract, Transform, Load) pipeline**.

  * **Extract:** The pipeline connects to multiple, live APIs (Reddit, Twitter/X, and YouTube) to pull in messy, raw data (posts, tweets, and comments).
  * **Transform:** It then uses `pandas` to clean, process, and standardize all this different data into one clean, useful format‚Äîall in memory.
  * **Load:** Finally, it loads all the processed data directly into a central `SQLite` database, ready for analysis.

## 2\. The "AI" Part: The Analysis Engine

This is where we add the intelligence. The pipeline takes the clean text and uses **Natural Language Processing (NLP)** to understand it.

  * **Sentiment Analysis:** It uses AI models (`VADER` and a custom-trained `scikit-learn` model) to read the text and classify its emotion as positive, negative, or neutral.
  * **Topic Modeling & NER:** It also uses `spaCy` and `NLTK` to automatically discover the main topics being discussed and find the most mentioned people and organizations.

## 3\. The Result: An Interactive Dashboard

All this data is useless if you can't see it. The final piece is a **web dashboard** built with `Plotly Dash`. It reads directly from our database and lets you:

  * Visualize all the insights in real-time.
  * Filter the data by platform (Reddit, Twitter, etc.).
  * Select a date range to see trends.

This project shows the complete journey: from messy, raw data on the internet to a clean, intelligent, and interactive dashboard.

-----

## üèóÔ∏è Project Architecture

This project is built from **14 key files** that work together as a system.

1.  **Configuration (`.env`, `.gitignore`, `requirements.txt`, `Procfile`)**

      * These files configure the project, hide secrets, and list dependencies.

2.  **Logic Scripts (`collect_*.py`, `database.py`, `process_data.py`, `sentiment_analysis.py`)**

      * These files **contain the functions** for our pipeline (e.g., `def collect_reddit_posts()`). They don't do anything when run directly.

3.  **Runner Scripts (`pipeline.py`, `dashboard.py`, `train_model.py`, `advanced_analysis.py`)**

      * These are the files you **actually run**. They `import` the functions from the logic scripts to perform complex tasks, like running the full ETL or launching the web app.

-----

## üíª Tech Stack

  * **Data Collection:** `praw` (Reddit), `tweepy` (Twitter), `google-api-python-client` (YouTube)
  * **Data Processing:** `pandas`
  * **NLP/ML:** `scikit-learn`, `spacy`, `nltk`, `vaderSentiment`, `joblib`
  * **Database:** `sqlite3`
  * **Dashboard:** `Plotly Dash`
  * **Automation:** `schedule`
  * **Deployment:** `gunicorn`

-----

## üèÉ‚Äç‚ôÇÔ∏è Installation & Setup (Windows Guide)

### 1\. Prerequisites

  * Install **Python** from [python.org/downloads](https://www.python.org/downloads/).
      * **IMPORTANT:** Check the box that says **"Add Python to PATH"** during installation.
  * Install **Git** from [git-scm.com/downloads](https://www.google.com/search?q=https://git-scm.com/downloads).

### 2\. Clone the Repository

Open **Command Prompt** (search for `cmd`).

```bash
:: Navigate to your Documents folder
cd Documents

:: Clone the project (replace with your GitHub URL)
git clone https://github.com/your_username/your_project_name.git
cd your_project_name
```

### 3\. Create and Activate Virtual Environment

```cmd
:: Create the environment
python -m venv venv

:: Activate the environment
venv\Scripts\activate.bat
```

*(You must run `venv\Scripts\activate.bat` every time you open a new terminal)*

### 4\. Install Dependencies

```bash
pip install -r requirements.txt
```

*(This uses the `requirements.txt` file you will create)*

### 5\. Download NLP Models

```bash
python -m spacy download en_core_web_sm
python -c "import nltk; nltk.download('stopwords'); nltk.download('vader_lexicon')"
```

### 6\. Create `.env` Secrets File

Create a file named `.env` in your project folder (you can use VS Code) and paste this in. Fill it with your own API keys.

```
# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_secret_here
REDDIT_USER_AGENT=MyAnalyticsApp/1.0

# Twitter/X API Credentials
TWITTER_BEARER_TOKEN=your_bearer_token_here

# YouTube API Credentials
YOUTUBE_API_KEY=your_google_api_key_here
```

-----

## üöÄ How to Run Your Project

### 1\. (Optional) Train the Custom Model

  * Download the [Sentiment140 dataset from Kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140).
  * Unzip it and rename the CSV file to `training_data.csv`.
  * Place it in your project folder and run:
    ```bash
    python train_model.py
    ```

*(This will create the `custom_sentiment_model.pkl` file)*

### 2\. Run the ETL Pipeline

This will collect data from all 3 platforms, clean it, analyze it, and save it to `social_media.db`.

```bash
python pipeline.py
```

*(Let it run once. When you see "Pipeline scheduled... Press Ctrl+C to stop", press **Ctrl+C**)*

### 3\. Run the Advanced Analysis (Optional)

To see the Topic Modeling and NER results, run:

```bash
python advanced_analysis.py
```

### 4\. Run the Dashboard

This starts your web application.

```bash
python dashboard.py
```

  * Open your browser and go to **`http://127.0.0.1:8050`** to see your dashboard.
